{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSvjwmu3vPMR"
   },
   "source": [
    "# Final Project - Reinforcements Learning \n",
    "Hello dear students,<br> this is the template notebook. Please click on the \"File\" tab and then on \"Save a copy into drive\".\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "### Name and ID:\n",
    "Student 1: Avraham Raviv, 204355390\n",
    "<br>\n",
    "Student 2: Yevgeni Berkovitch, 317079234\n",
    "<br><br>\n",
    "<img src=\"https://play-lh.googleusercontent.com/e_oKlKPISbgdzut1H9opevS7-LTB8-8lsmpCdMkhlnqFenZhpjxbLmx7l158-xQQCIY\">\n",
    "\n",
    "### https://github.com/mpSchrader/gym-sokoban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T3qcykHFi15"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2dah0RrY9Kmj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install gym\n",
    "!pip install pygame\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet\n",
    "!pip install gym\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install gym_sokoban\n",
    "\n",
    "!imageio_download_bin ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHbKbI7BwIwv"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1cNdWkV49OqN"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from soko_pap import *\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pa3tRhUfzEJ4"
   },
   "outputs": [],
   "source": [
    "from gym import logger as gymlogger\n",
    "gymlogger.set_level(40) # error only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7bJeRHbwMIj"
   },
   "source": [
    "# Display utils\n",
    "The cell below contains the video display configuration. No need to make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z41WGwQt9i7_"
   },
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances_for_target(room_state, target):\n",
    "    distances = np.zeros(shape=room_state.shape)\n",
    "    visited_cells = set()\n",
    "    cell_queue = deque()\n",
    "\n",
    "    visited_cells.add(target)\n",
    "    cell_queue.appendleft(target)\n",
    "\n",
    "    while len(cell_queue) != 0:\n",
    "        cell = cell_queue.pop()\n",
    "        distance = distances[cell[0]][cell[1]]\n",
    "        for x,y in ((1,0), (-1,-0), (0,1), (0,-1)):\n",
    "            next_cell_x, next_cell_y = cell[0]+x, cell[1]+y\n",
    "            if room_state[next_cell_x][next_cell_y] != 0 and not (next_cell_x, next_cell_y) in visited_cells:\n",
    "                distances[next_cell_x][next_cell_y] = distance + 1\n",
    "                visited_cells.add((next_cell_x, next_cell_y))\n",
    "                cell_queue.appendleft((next_cell_x, next_cell_y))\n",
    "                \n",
    "    return distances\n",
    "\n",
    "def get_maze_info(room_state):\n",
    "    targets = []\n",
    "    for i in range(room_state.shape[0]):\n",
    "        for j in range(room_state.shape[1]):\n",
    "            if room_state[i][j] == 2:\n",
    "                targets.append((i, j))\n",
    "\n",
    "    distances0 = get_distances_for_target(room_state, targets[0])\n",
    "    distances1 = get_distances_for_target(room_state, targets[1])\n",
    "    common_distances = np.minimum(distances0, distances1)\n",
    "    \n",
    "    maze_info = {}\n",
    "    maze_info['target0'] = targets[0]\n",
    "    maze_info['target1'] = targets[1]\n",
    "    maze_info['distances0'] = distances0\n",
    "    maze_info['distances1'] = distances1\n",
    "    maze_info['common_distances'] = common_distances\n",
    "    return maze_info\n",
    "\n",
    "def calc_distances(room_state, distances):\n",
    "    boxes = []\n",
    "    for i in range(room_state.shape[0]):\n",
    "        for j in range(room_state.shape[1]):            \n",
    "            if room_state[i][j] == 4:\n",
    "                boxes.append((i,j))\n",
    "    if len(boxes) == 2:\n",
    "        return distances[boxes[0][0]][boxes[0][1]] + distances[boxes[1][0]][boxes[1][1]]\n",
    "    \n",
    "    return distances[boxes[0][0]][boxes[0][1]]\n",
    "\n",
    "def box2target_change_reward(room_state, next_room_state, maze_info):\n",
    "    if np.array_equal(room_state, next_room_state):\n",
    "        return -5.0\n",
    "    \n",
    "    target0 = maze_info['target0']\n",
    "    target1 = maze_info['target1']\n",
    "    distances0 = maze_info['distances0']\n",
    "    distances1 = maze_info['distances1']\n",
    "    common_distances = maze_info['common_distances']\n",
    "    \n",
    "    relevant_distances = common_distances    \n",
    "    \n",
    "    if room_state[target0[0]][target0[1]] == 3:\n",
    "        relevant_distances = distances1\n",
    "    elif room_state[target1[0]][target1[1]] == 3:\n",
    "        relevant_distances = distances0\n",
    "    \n",
    "    change_reward = 0.0      \n",
    "    t2b = calc_distances(room_state, relevant_distances)\n",
    "    n_t2b = calc_distances(next_room_state, relevant_distances)\n",
    "    if n_t2b < t2b:\n",
    "        change_reward += 5.0\n",
    "    elif n_t2b > t2b:\n",
    "        change_reward -= 5.0\n",
    "        \n",
    "    return change_reward  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6Qnw883yqGH"
   },
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gGl0DQvSQG0d"
   },
   "outputs": [],
   "source": [
    "class SOK_Agent:\n",
    "    def __init__(self):\n",
    "        # Construct DQN models\n",
    "        self.state_size = (112,112,1) \n",
    "        self.action_size = 8\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.batch_size = 8\n",
    "        \n",
    "        # Replay buffers\n",
    "        self.replay_buffer = deque(maxlen=10000)\n",
    "        self.prioritized_replay_buffer = deque(maxlen=1000)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0   \n",
    "        self.epsilon_min = 0.2\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.replay_rate = 10\n",
    "        self.update_beta = 0.999\n",
    "        \n",
    "        self.action_rotation_map = {\n",
    "            0: 2,\n",
    "            1: 3,\n",
    "            2: 1,\n",
    "            3: 0,\n",
    "            4: 6,\n",
    "            5: 7,\n",
    "            6: 5,\n",
    "            7: 4\n",
    "        }\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (16,16), strides=(16,16), input_shape=self.state_size, activation='relu'))\n",
    "        model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))  \n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        lr_schedule = ExponentialDecay(0.001, decay_steps=2000, decay_rate=0.99, staircase=False)\n",
    "        model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='mse')        \n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append([state, action, reward, next_state, done])    \n",
    "        \n",
    "    def copy_to_prioritized_buffer(self, n):\n",
    "        for i in range(n):\n",
    "            self.prioritized_replay_buffer.append(self.replay_buffer[-1-i])  \n",
    "\n",
    "    def act(self, state, stochastic=False):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state, verbose=0)[0]\n",
    "        \n",
    "        if stochastic:\n",
    "            act_probs = np.exp(act_values)/np.exp(act_values).sum()\n",
    "            return np.random.choice(np.arange(self.action_size), size=1, p=act_probs)[0]\n",
    "              \n",
    "        return np.argmax(act_values) \n",
    "\n",
    "    def replay(self): \n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        if len(self.prioritized_replay_buffer) < self.batch_size//2:\n",
    "            minibatch = random.sample(self.replay_buffer, self.batch_size) \n",
    "        else:    \n",
    "            minibatch = random.sample(self.replay_buffer, self.batch_size//2) \n",
    "            minibatch.extend(random.sample(self.prioritized_replay_buffer, self.batch_size//2))\n",
    "        \n",
    "        states = np.zeros((self.batch_size*4, self.state_size[0], self.state_size[1]))\n",
    "        actions = np.zeros(self.batch_size*4, dtype=int)\n",
    "        rewards = np.zeros(self.batch_size*4)\n",
    "        next_states = np.zeros((self.batch_size*4, self.state_size[0], self.state_size[1]))\n",
    "        statuses = np.zeros(self.batch_size*4)\n",
    "        targets = np.zeros((self.batch_size*4, self.action_size)) \n",
    "        \n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch): \n",
    "            for rot in range(4):  \n",
    "                ind = i*4+rot\n",
    "                if rot != 0:\n",
    "                    state = np.rot90(state, axes=(1,2))\n",
    "                    next_state = np.rot90(next_state, axes=(1,2))\n",
    "                    action = self.action_rotation_map.get(action)\n",
    "\n",
    "                states[ind] = state.copy()\n",
    "                actions[ind] = action\n",
    "                rewards[ind] = reward\n",
    "                next_states[ind] = next_state.copy()\n",
    "                statuses[ind] = 1 if done else 0          \n",
    "        \n",
    "        targets = self.model.predict(states) \n",
    "        max_actions = np.argmax(self.model.predict(next_states), axis=1)\n",
    "        next_rewards = self.target_model.predict(next_states)\n",
    "        \n",
    "        ind = 0\n",
    "        for action, reward, next_reward, max_action, done in zip(actions, rewards, next_rewards, max_actions, statuses):  \n",
    "            if not done:\n",
    "                reward += self.gamma * next_reward[max_action]\n",
    "            targets[ind][action] = reward\n",
    "            ind += 1\n",
    "        \n",
    "        self.model.fit(states, targets, epochs=10, verbose=0) \n",
    "        \n",
    "        self.update_target_model()        \n",
    "    \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay    \n",
    "        \n",
    "    def update_target_model(self):\n",
    "        model_w = self.model.get_weights()\n",
    "        target_model_w = self.target_model.get_weights()\n",
    "        updated_target_model_w = []\n",
    "        for i in range(len(model_w)):\n",
    "            updated_target_model_w.append(self.update_beta*target_model_w[i] + (1-self.update_beta)*model_w[i])\n",
    "        self.target_model.set_weights(updated_target_model_w)    \n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        self.target_model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "I8TqYhBX1lD4"
   },
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    f = frame.mean(axis=2)\n",
    "    f = f / 255\n",
    "    return np.expand_dims(f, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 50000\n",
    "max_steps = 20\n",
    "\n",
    "def init_sok(r):\n",
    "    random.seed(r)\n",
    "    sok = PushAndPullSokobanEnv(dim_room=(7, 7), num_boxes=1)\n",
    "    sok.set_maxsteps(max_steps)\n",
    "    return sok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 1\n",
      "2 | 2\n",
      "3 | 3\n",
      "4 | 4\n",
      "5 | 5\n",
      "6 | 6\n",
      "7 | 7\n",
      "8 | 8\n",
      "9 | 9\n",
      "10 | 10\n",
      "11 | 11\n",
      "12 | 12\n",
      "13 | 13\n",
      "14 | 14\n",
      "15 | 15\n",
      "16 | 16\n",
      "17 | 17\n",
      "18 | 18\n",
      "19 | 19\n",
      "20 | 20\n",
      "21 | 21\n",
      "22 | 22\n",
      "23 | 23\n",
      "24 | 24\n",
      "24 | 25\n",
      "25 | 26\n",
      "25 | 27\n",
      "26 | 28\n",
      "27 | 29\n",
      "28 | 30\n",
      "29 | 31\n",
      "30 | 32\n",
      "31 | 33\n",
      "32 | 34\n",
      "33 | 35\n",
      "34 | 36\n",
      "35 | 37\n",
      "36 | 38\n",
      "36 | 39\n",
      "37 | 40\n",
      "38 | 41\n",
      "39 | 42\n",
      "40 | 43\n",
      "41 | 44\n",
      "41 | 45\n",
      "42 | 46\n",
      "43 | 47\n",
      "44 | 48\n",
      "45 | 49\n",
      "45 | 50\n",
      "45 | 51\n",
      "46 | 52\n",
      "47 | 53\n",
      "48 | 54\n",
      "49 | 55\n",
      "50 | 56\n",
      "51 | 57\n",
      "52 | 58\n",
      "53 | 59\n",
      "54 | 60\n",
      "55 | 61\n",
      "56 | 62\n",
      "57 | 63\n",
      "58 | 64\n",
      "59 | 65\n",
      "59 | 66\n",
      "60 | 67\n",
      "61 | 68\n",
      "61 | 69\n",
      "62 | 70\n",
      "63 | 71\n",
      "64 | 72\n",
      "65 | 73\n",
      "66 | 74\n",
      "67 | 75\n",
      "68 | 76\n",
      "69 | 77\n",
      "70 | 78\n",
      "71 | 79\n",
      "72 | 80\n",
      "73 | 81\n",
      "73 | 82\n",
      "74 | 83\n",
      "75 | 84\n",
      "76 | 85\n",
      "77 | 86\n",
      "78 | 87\n",
      "79 | 88\n",
      "80 | 89\n",
      "81 | 90\n",
      "82 | 91\n",
      "83 | 92\n",
      "84 | 93\n",
      "85 | 94\n",
      "86 | 95\n",
      "86 | 96\n",
      "87 | 97\n",
      "88 | 98\n",
      "89 | 99\n",
      "90 | 100\n"
     ]
    }
   ],
   "source": [
    "agent = SOK_Agent()\n",
    "agent.load(\"models\\Q3_06A_69.h5\")\n",
    "\n",
    "agent.epsilon = 0.0\n",
    "num_solved = 0\n",
    "\n",
    "for t in range(100):    \n",
    "    sok = init_sok(t)\n",
    "    steps = 0\n",
    "\n",
    "    state = sok.get_image('rgb_array')\n",
    "    done = False\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        action = agent.act(process_frame(state))\n",
    "        if action < 4:\n",
    "            action += 1\n",
    "        else:\n",
    "            action += 5\n",
    "        state, reward, done, info = sok.step(action)\n",
    "\n",
    "    #if 3 in sok.room_state:\n",
    "    if 3 in sok.room_state:\n",
    "        num_solved += 1\n",
    "        \n",
    "    print(\"%d | %d\" % (num_solved, t+1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
