{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSvjwmu3vPMR"
   },
   "source": [
    "# Final Project - Reinforcements Learning \n",
    "Hello dear students,<br> this is the template notebook. Please click on the \"File\" tab and then on \"Save a copy into drive\".\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "### Name and ID:\n",
    "Student 1: Avraham Raviv, 204355390\n",
    "<br>\n",
    "Student 2: Yevgeni Berkovitch, 317079234\n",
    "<br><br>\n",
    "<img src=\"https://play-lh.googleusercontent.com/e_oKlKPISbgdzut1H9opevS7-LTB8-8lsmpCdMkhlnqFenZhpjxbLmx7l158-xQQCIY\">\n",
    "\n",
    "### https://github.com/mpSchrader/gym-sokoban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T3qcykHFi15"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2dah0RrY9Kmj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install gym\n",
    "!pip install pygame\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet\n",
    "!pip install gym\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install gym_sokoban\n",
    "\n",
    "!imageio_download_bin ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHbKbI7BwIwv"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1cNdWkV49OqN"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from soko_pap import *\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TxfvY69Czk_n"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pa3tRhUfzEJ4"
   },
   "outputs": [],
   "source": [
    "from gym import logger as gymlogger\n",
    "gymlogger.set_level(40) # error only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7bJeRHbwMIj"
   },
   "source": [
    "# Display utils\n",
    "The cell below contains the video display configuration. No need to make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "z41WGwQt9i7_"
   },
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances_for_target(room_state, target):\n",
    "    distances = np.zeros(shape=room_state.shape)\n",
    "    visited_cells = set()\n",
    "    cell_queue = deque()\n",
    "\n",
    "    visited_cells.add(target)\n",
    "    cell_queue.appendleft(target)\n",
    "\n",
    "    while len(cell_queue) != 0:\n",
    "        cell = cell_queue.pop()\n",
    "        distance = distances[cell[0]][cell[1]]\n",
    "        for x,y in ((1,0), (-1,-0), (0,1), (0,-1)):\n",
    "            next_cell_x, next_cell_y = cell[0]+x, cell[1]+y\n",
    "            if room_state[next_cell_x][next_cell_y] != 0 and not (next_cell_x, next_cell_y) in visited_cells:\n",
    "                distances[next_cell_x][next_cell_y] = distance + 1\n",
    "                visited_cells.add((next_cell_x, next_cell_y))\n",
    "                cell_queue.appendleft((next_cell_x, next_cell_y))\n",
    "                \n",
    "    return distances\n",
    "\n",
    "def get_distances(room_state):\n",
    "    targets = []\n",
    "    for i in range(room_state.shape[0]):\n",
    "        for j in range(room_state.shape[1]):\n",
    "            if room_state[i][j] in (2, 3):\n",
    "                targets.append((i, j))\n",
    "\n",
    "    distances1 = get_distances_for_target(room_state, targets[0])\n",
    "    distances2 = get_distances_for_target(room_state, targets[1])\n",
    "    return np.minimum(distances1, distances2)\n",
    "\n",
    "def calc_distances(room_state, distances):\n",
    "    boxes = []\n",
    "    for i in range(room_state.shape[0]):\n",
    "        for j in range(room_state.shape[1]):            \n",
    "            if room_state[i][j] in (3,4):\n",
    "                boxes.append((i,j))\n",
    "    \n",
    "    return distances[boxes[0][0]][boxes[0][1]] + distances[boxes[1][0]][boxes[1][1]]   \n",
    "\n",
    "def box2target_change_reward(room_state, next_room_state, distances):\n",
    "    if np.array_equal(room_state, next_room_state):\n",
    "        return -5.0\n",
    "    \n",
    "    t2b = calc_distances(room_state, distances)\n",
    "    n_t2b = calc_distances(next_room_state, distances)\n",
    "    \n",
    "    change_reward = 0.0\n",
    "    if n_t2b < t2b:\n",
    "        change_reward += 5.0\n",
    "    elif n_t2b > t2b:\n",
    "        change_reward -= 5.0\n",
    "        \n",
    "    return change_reward   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6Qnw883yqGH"
   },
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gGl0DQvSQG0d"
   },
   "outputs": [],
   "source": [
    "class SOK_Agent:\n",
    "    def __init__(self):\n",
    "        # Construct DQN models\n",
    "        self.state_size = (112,112,1) \n",
    "        self.action_size = 8\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.batch_size = 8\n",
    "        \n",
    "        # Replay buffers\n",
    "        self.replay_buffer = deque(maxlen=50000)\n",
    "        self.prioritized_replay_buffer = deque(maxlen=50000)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0   \n",
    "        self.epsilon_min = 0.3\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.replay_rate = 10\n",
    "        self.update_beta = 0.99\n",
    "        \n",
    "        self.action_rotation_map = {\n",
    "            0: 2,\n",
    "            1: 3,\n",
    "            2: 1,\n",
    "            3: 0,\n",
    "            4: 6,\n",
    "            5: 7,\n",
    "            6: 5,\n",
    "            7: 4\n",
    "        }\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (16,16), strides=(16,16), input_shape=self.state_size, activation='relu'))\n",
    "        model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))    \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=\"adam\")        \n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append([state, action, reward, next_state, done])    \n",
    "        \n",
    "    def copy_to_prioritized_buffer(self, n):\n",
    "        for i in range(n):\n",
    "            self.prioritized_replay_buffer.append(self.replay_buffer[-1-i])  \n",
    "\n",
    "    def act(self, state, stochastic=False):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state, verbose=0)[0]\n",
    "        \n",
    "        if stochastic:\n",
    "            act_probs = np.exp(act_values)/np.exp(act_values).sum()\n",
    "            return np.random.choice(np.arange(self.action_size), size=1, p=act_probs)[0]\n",
    "              \n",
    "        return np.argmax(act_values) \n",
    "\n",
    "    def replay(self): \n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        if len(self.prioritized_replay_buffer) < self.batch_size//2:\n",
    "            minibatch = random.sample(self.replay_buffer, self.batch_size) \n",
    "        else:    \n",
    "            minibatch = random.sample(self.replay_buffer, self.batch_size//2) \n",
    "            minibatch.extend(random.sample(self.prioritized_replay_buffer, self.batch_size//2))\n",
    "        \n",
    "        states = np.zeros((self.batch_size*4, self.state_size[0], self.state_size[1]))\n",
    "        actions = np.zeros(self.batch_size*4, dtype=int)\n",
    "        rewards = np.zeros(self.batch_size*4)\n",
    "        next_states = np.zeros((self.batch_size*4, self.state_size[0], self.state_size[1]))\n",
    "        statuses = np.zeros(self.batch_size*4)\n",
    "        targets = np.zeros((self.batch_size*4, self.action_size)) \n",
    "        \n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch): \n",
    "            for rot in range(4):  \n",
    "                ind = i*4+rot\n",
    "                if rot != 0:\n",
    "                    state = np.rot90(state, axes=(1,2))\n",
    "                    next_state = np.rot90(next_state, axes=(1,2))\n",
    "                    action = self.action_rotation_map.get(action)\n",
    "\n",
    "                states[ind] = state.copy()\n",
    "                actions[ind] = action\n",
    "                rewards[ind] = reward\n",
    "                next_states[ind] = next_state.copy()\n",
    "                statuses[ind] = 1 if done else 0          \n",
    "        \n",
    "        targets = self.model.predict(states) \n",
    "        max_actions = np.argmax(self.model.predict(next_states), axis=1)\n",
    "        next_rewards = self.target_model.predict(next_states)\n",
    "        \n",
    "        ind = 0\n",
    "        for action, reward, next_reward, max_action, done in zip(actions, rewards, next_rewards, max_actions, statuses):  \n",
    "            if not done:\n",
    "                reward += self.gamma * next_reward[max_action]\n",
    "            targets[ind][action] = reward\n",
    "            ind += 1\n",
    "        \n",
    "        self.model.fit(states, targets, epochs=10, verbose=0) \n",
    "        \n",
    "        self.update_target_model()        \n",
    "    \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay    \n",
    "        \n",
    "    def update_target_model(self):\n",
    "        model_w = self.model.get_weights()\n",
    "        target_model_w = self.target_model.get_weights()\n",
    "        updated_target_model_w = []\n",
    "        for i in range(len(model_w)):\n",
    "            updated_target_model_w.append(self.update_beta*target_model_w[i] + (1-self.update_beta)*model_w[i])\n",
    "        self.target_model.set_weights(updated_target_model_w)    \n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "I8TqYhBX1lD4"
   },
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    f = frame.mean(axis=2)\n",
    "    f = f / 255\n",
    "    return np.expand_dims(f, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 50000\n",
    "max_steps = 50\n",
    "\n",
    "def init_sok(r):\n",
    "    random.seed(r)\n",
    "    sok = PushAndPullSokobanEnv(dim_room=(7, 7), num_boxes=2)\n",
    "    sok.set_maxsteps(max_steps)\n",
    "    return sok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(e, stochastic=False):\n",
    "    current_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    num_solved = 0\n",
    "    solved_in_steps = defaultdict(int)\n",
    "\n",
    "    for t in tqdm(range(100)):    \n",
    "        sok = init_sok(t)\n",
    "        steps = 0\n",
    "\n",
    "        state = sok.get_image('rgb_array')\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps += 1\n",
    "            action = agent.act(process_frame(state), stochastic)\n",
    "            if action < 4:\n",
    "                action += 1\n",
    "            else:\n",
    "                action += 5\n",
    "            state, reward, done, info = sok.step(action)\n",
    "\n",
    "        if 3 in sok.room_state:            \n",
    "            num_solved += 1\n",
    "            solved_in_steps[steps] += 1\n",
    "    \n",
    "    agent.epsilon = current_epsilon    \n",
    "    print(\"Episode %d Solved: %d\" % (e+1, num_solved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s2Km5jCqDqbz",
    "outputId": "abab4d45-7d3a-4a49-8b54-10c34549249d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 10\n",
      "2 | 20\n",
      "2 | 30\n",
      "2 | 40\n",
      "2 | 50\n",
      "2 | 60\n",
      "3 | 70\n",
      "3 | 80\n",
      "6 | 90\n",
      "7 | 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a598160a6a12493a9359405f56187b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOKOBAN] Runtime Error/Warning: Not enough free spots (#3) to place 1 player and 2 boxes.\n",
      "[SOKOBAN] Retry . . .\n",
      "\n",
      "Episode 100 Solved: 21\n",
      "0 | 10\n",
      "0 | 20\n",
      "0 | 30\n",
      "2 | 40\n",
      "2 | 50\n",
      "4 | 60\n",
      "4 | 70\n",
      "4 | 80\n",
      "6 | 90\n",
      "[SOKOBAN] Runtime Error/Warning: Generated Model with score == 0\n",
      "[SOKOBAN] Retry . . .\n",
      "6 | 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5544a55d8a4728928595164a37dce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOKOBAN] Runtime Error/Warning: Not enough free spots (#3) to place 1 player and 2 boxes.\n",
      "[SOKOBAN] Retry . . .\n",
      "\n",
      "Episode 200 Solved: 22\n",
      "2 | 10\n",
      "4 | 20\n",
      "4 | 30\n",
      "5 | 40\n",
      "7 | 50\n",
      "[SOKOBAN] Runtime Error/Warning: Generated Model with score == 0\n",
      "[SOKOBAN] Retry . . .\n",
      "7 | 60\n",
      "[SOKOBAN] Runtime Error/Warning: Not enough free spots (#3) to place 1 player and 2 boxes.\n",
      "[SOKOBAN] Retry . . .\n",
      "7 | 70\n",
      "7 | 80\n",
      "7 | 90\n"
     ]
    }
   ],
   "source": [
    "agent = SOK_Agent()\n",
    "\n",
    "running_puzzles = 0\n",
    "running_solved = 0\n",
    "\n",
    "for e in range(max_episodes):\n",
    "    sok = init_sok(e+100)\n",
    "    random.seed(e)\n",
    "    running_puzzles += 1\n",
    "    \n",
    "    state = process_frame(sok.get_image('rgb_array'))\n",
    "    room_state = sok.room_state.copy() \n",
    "    distances = get_distances(room_state)\n",
    "    \n",
    "    for step in range(sok.max_steps):\n",
    "        action = agent.act(state)\n",
    "        if action < 4:\n",
    "            next_state, reward, done, _ = sok.step(action+1) \n",
    "        else:\n",
    "            next_state, reward, done, _ = sok.step(action+5)         \n",
    "        \n",
    "        next_state = process_frame(next_state)        \n",
    "        next_room_state = sok.room_state\n",
    "        \n",
    "        if not done:\n",
    "            reward += box2target_change_reward(room_state, next_room_state, distances)\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state.copy() \n",
    "        room_state = next_room_state.copy()                \n",
    "        \n",
    "        if (step+1) % agent.replay_rate == 0:\n",
    "            agent.replay()            \n",
    "        \n",
    "        if done: \n",
    "            if sok.boxes_on_target == 2:  \n",
    "                agent.copy_to_prioritized_buffer(step+1)  \n",
    "                running_solved += 1\n",
    "                \n",
    "            if (e+1) % 10 == 0 and e > 0:\n",
    "                print(f\"{running_solved} | {running_puzzles}\") \n",
    "\n",
    "                if (e+1) % 100 == 0:\n",
    "                    running_puzzles = 0\n",
    "                    running_solved = 0\n",
    "                    \n",
    "            break\n",
    "            \n",
    "    if (e+1) % 100 == 0 and e > 0:\n",
    "        test_agent(e, stochastic=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
