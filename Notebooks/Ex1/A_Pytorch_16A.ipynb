{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSvjwmu3vPMR"
   },
   "source": [
    "# Final Project - Reinforcements Learning \n",
    "Hello dear students,<br> this is the template notebook. Please click on the \"File\" tab and then on \"Save a copy into drive\".\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "### Name and ID:\n",
    "Student 1: Avraham Raviv, 204355390\n",
    "<br>\n",
    "Student 2: Yevgeni Berkovitch, 317079234\n",
    "<br><br>\n",
    "<img src=\"https://play-lh.googleusercontent.com/e_oKlKPISbgdzut1H9opevS7-LTB8-8lsmpCdMkhlnqFenZhpjxbLmx7l158-xQQCIY\">\n",
    "\n",
    "### https://github.com/mpSchrader/gym-sokoban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T3qcykHFi15"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2dah0RrY9Kmj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install gym\n",
    "!pip install pygame\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet\n",
    "!pip install gym\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install gym_sokoban\n",
    "\n",
    "!imageio_download_bin ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHbKbI7BwIwv"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1cNdWkV49OqN"
   },
   "outputs": [],
   "source": [
    "from soko_pap import *\n",
    "\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TxfvY69Czk_n"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pa3tRhUfzEJ4"
   },
   "outputs": [],
   "source": [
    "from gym import logger as gymlogger\n",
    "gymlogger.set_level(40) # error only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7bJeRHbwMIj"
   },
   "source": [
    "# Display utils\n",
    "The cell below contains the video display configuration. No need to make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z41WGwQt9i7_"
   },
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distances(room_state):\n",
    "    for i in range(room_state.shape[0]):\n",
    "        for j in range(room_state.shape[1]):\n",
    "            if room_state[i][j] == 2:\n",
    "                target = (i, j)\n",
    "\n",
    "    distances = np.zeros(shape=room_state.shape)\n",
    "    visited_cells = set()\n",
    "    cell_queue = deque()\n",
    "\n",
    "    visited_cells.add(target)\n",
    "    cell_queue.appendleft(target)\n",
    "\n",
    "    while len(cell_queue) != 0:\n",
    "        cell = cell_queue.pop()\n",
    "        distance = distances[cell[0]][cell[1]]\n",
    "        for x,y in ((1,0), (-1,-0), (0,1), (0,-1)):\n",
    "            next_cell_x, next_cell_y = cell[0]+x, cell[1]+y\n",
    "            if room_state[next_cell_x][next_cell_y] != 0 and not (next_cell_x, next_cell_y) in visited_cells:\n",
    "                distances[next_cell_x][next_cell_y] = distance + 1\n",
    "                visited_cells.add((next_cell_x, next_cell_y))\n",
    "                cell_queue.appendleft((next_cell_x, next_cell_y))\n",
    "                \n",
    "    return distances   \n",
    "\n",
    "def fetch_distances(room_state, distances):\n",
    "    box = None\n",
    "    mover = None\n",
    "    for i in range(room_state.shape[0]):\n",
    "        for j in range(room_state.shape[1]):            \n",
    "            if room_state[i][j] == 4:\n",
    "                box = (i,j)\n",
    "            \n",
    "            if room_state[i][j] == 5:\n",
    "                mover = (i,j)\n",
    "    \n",
    "    return mover, box, distances[box[0]][box[1]]   \n",
    "\n",
    "def box2target_change_reward(room_state, next_room_state, distances):\n",
    "    if np.array_equal(room_state, next_room_state):\n",
    "        return -1.0\n",
    "    \n",
    "    mover, box, t2b = fetch_distances(room_state, distances)\n",
    "    n_mover, n_box, n_t2b = fetch_distances(next_room_state, distances)\n",
    "    \n",
    "    change_reward = 0.0\n",
    "    if n_t2b < t2b:\n",
    "        change_reward += 5.0\n",
    "    elif n_t2b > t2b:\n",
    "        change_reward -= 5.0\n",
    "        \n",
    "    m2b = np.sqrt((mover[0]-box[0])**2 + (mover[1]-box[1])**2)\n",
    "    n_m2b = np.sqrt((n_mover[0]-n_box[0])**2 + (n_mover[1]-n_box[1])**2)\n",
    "    \n",
    "    if n_m2b < m2b and m2b >= 2:\n",
    "        change_reward += 1.0\n",
    "    elif n_m2b > m2b and n_m2b >= 2:\n",
    "        change_reward -= 1.0\n",
    "        \n",
    "    return change_reward  \n",
    "\n",
    "def process_frame(frame):\n",
    "    f = frame.mean(axis=2)\n",
    "    f = f / 255\n",
    "    return np.expand_dims(f, axis=0)\n",
    "\n",
    "action_rotation_map = {\n",
    "    0: 2,\n",
    "    1: 3,\n",
    "    2: 1,\n",
    "    3: 0,\n",
    "    4: 6,\n",
    "    5: 7,\n",
    "    6: 5,\n",
    "    7: 4\n",
    "    }\n",
    "\n",
    "\n",
    "def calc_rot_action(org_action, aug):\n",
    "    # given an action and an augmentation, return the action that is equivalent to the augmentation\n",
    "    # action is number from range 0-action_size, and aug is number from range 1-3\n",
    "    # for single augmentation step, the action is change by using action_rotation_map\n",
    "    action = org_action\n",
    "    for i in range(aug):\n",
    "        action = action_rotation_map[action]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6Qnw883yqGH"
   },
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sokoban_DNN_Model(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Sokoban_DNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_size[2], 32, kernel_size=16, stride=16)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ReLU(self.conv1(x))\n",
    "        x = self.ReLU(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.ReLU(self.fc1(x))\n",
    "        x = self.ReLU(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gGl0DQvSQG0d"
   },
   "outputs": [],
   "source": [
    "class SOK_Agent:\n",
    "    def __init__(self):\n",
    "        # Construct DQN models\n",
    "        self.state_size = (112, 112, 1)\n",
    "        self.action_size = 8\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.batch_size = 8\n",
    "\n",
    "        # Replay buffers\n",
    "        self.replay_buffer = deque(maxlen=5000)\n",
    "        self.prioritized_replay_buffer = deque(maxlen=500)\n",
    "\n",
    "        # Hyper parameters\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.3\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.replay_rate = 10\n",
    "        self.update_beta = 0.9999\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        # info\n",
    "        self.max_steps = 20\n",
    "        self.max_episodes = 50000\n",
    "        self.solved = 0\n",
    "        self.test_rate = 100\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sokoban_DNN_Model(self.state_size, self.action_size)\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append([state, action, reward, next_state, done])\n",
    "\n",
    "    def copy_to_prioritized_buffer(self, n):\n",
    "        for i in range(n):\n",
    "            self.prioritized_replay_buffer.append(self.replay_buffer[-1 - i])\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        act_values = self.model(state_tensor.unsqueeze(0)).detach()[0]\n",
    "        return act_values.argmax().item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        if len(self.prioritized_replay_buffer) < self.batch_size // 2:\n",
    "            minibatch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        else:\n",
    "            minibatch = random.sample(self.replay_buffer, self.batch_size // 2)\n",
    "            minibatch.extend(random.sample(self.prioritized_replay_buffer, self.batch_size // 2))\n",
    "\n",
    "        states = torch.zeros((self.batch_size*4, 1, self.state_size[0], self.state_size[1]))\n",
    "        actions = torch.zeros(self.batch_size*4)\n",
    "        rewards = torch.zeros(self.batch_size*4)\n",
    "        next_states = torch.zeros((self.batch_size*4, 1, self.state_size[0], self.state_size[1]))\n",
    "        statuses = torch.zeros(self.batch_size*4)        \n",
    "        \n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
    "            for aug in range(4):\n",
    "                ind = i*4+aug               \n",
    "                org_state = state_tensor.clone().detach()\n",
    "                states[ind] = torch.rot90(org_state, k=aug, dims=[1, 2])\n",
    "                actions[ind] = calc_rot_action(action, aug)\n",
    "                rewards[ind] = reward\n",
    "                org_next_state = next_state_tensor.clone().detach()\n",
    "                next_states[ind] = torch.rot90(org_next_state, k=aug, dims=[1, 2])\n",
    "                statuses[ind] = 1 if done else 0\n",
    "\n",
    "        # targets = self.model.predict(states)\n",
    "        # max_actions = np.argmax(self.model.predict(next_states), axis=1)\n",
    "        # next_rewards = self.target_model.predict(next_states)\n",
    "        targets = self.model(states).detach()\n",
    "        max_actions = torch.argmax(self.model(next_states).detach(), dim=1)\n",
    "        next_rewards = self.target_model(next_states).detach()\n",
    "\n",
    "        ind = 0\n",
    "        for action, reward, next_reward, max_action, done in zip(actions, rewards, next_rewards, max_actions, statuses):\n",
    "            if not done:\n",
    "                reward += self.gamma * next_reward[max_action]\n",
    "            targets[ind][action.int().item()] = reward.long()\n",
    "            ind += 1\n",
    "\n",
    "        for _ in range(10):    \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = torch.nn.functional.mse_loss(self.model(states), targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_target_model(self):\n",
    "        model_dict = self.model.state_dict()\n",
    "        target_dict = self.target_model.state_dict()\n",
    "        for key in model_dict:\n",
    "            target_dict[key] = self.update_beta * target_dict[key] + (1 - self.update_beta) * model_dict[key]\n",
    "\n",
    "    def test_agent(self, e, stochastic=False):\n",
    "        current_epsilon = self.epsilon\n",
    "        self.epsilon = 0.0\n",
    "        num_solved = 0\n",
    "        solved_in_steps = defaultdict(int)\n",
    "\n",
    "        for t in range(100):\n",
    "            random.seed(t)\n",
    "            sok = PushAndPullSokobanEnv(dim_room=(7, 7), num_boxes=1)\n",
    "            sok.set_maxsteps(self.max_steps)\n",
    "            steps = 0\n",
    "\n",
    "            state = sok.get_image('rgb_array')\n",
    "            done = False\n",
    "            while not done:\n",
    "                steps += 1\n",
    "                action = self.act(process_frame(state))\n",
    "                if action < 4:\n",
    "                    action += 1\n",
    "                else:\n",
    "                    action += 5\n",
    "                state, reward, done, info = sok.step(action)\n",
    "\n",
    "            if 3 in sok.room_state:\n",
    "                num_solved += 1\n",
    "                solved_in_steps[steps] += 1\n",
    "        \n",
    "        print(\"Episode %d Solved: %d (%s)\" % (e, num_solved, \"Stochastic\" if stochastic else \"Deterministic\"))\n",
    "\n",
    "        return num_solved\n",
    "\n",
    "    def init_sok(self, r):\n",
    "        random.seed(r + 100)\n",
    "        Sok = PushAndPullSokobanEnv(dim_room=(7, 7), num_boxes=1)\n",
    "        Sok.set_maxsteps(self.max_steps)\n",
    "        return Sok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s2Km5jCqDqbz",
    "outputId": "abab4d45-7d3a-4a49-8b54-10c34549249d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Episode 100 Solved: 39 (Deterministic)\n",
      "Episode 200 Solved: 43 (Deterministic)\n",
      "Episode 300 Solved: 47 (Deterministic)\n",
      "Episode 400 Solved: 47 (Deterministic)\n",
      "Episode 500 Solved: 49 (Deterministic)\n",
      "Episode 600 Solved: 52 (Deterministic)\n",
      "Episode 700 Solved: 61 (Deterministic)\n",
      "Episode 800 Solved: 62 (Deterministic)\n",
      "Episode 900 Solved: 61 (Deterministic)\n",
      "Episode 1000 Solved: 60 (Deterministic)\n",
      "Episode 1100 Solved: 60 (Deterministic)\n",
      "Episode 1200 Solved: 60 (Deterministic)\n",
      "Episode 1300 Solved: 65 (Deterministic)\n",
      "Episode 1400 Solved: 65 (Deterministic)\n",
      "Episode 1500 Solved: 59 (Deterministic)\n",
      "Episode 1600 Solved: 62 (Deterministic)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-522624166276>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3ab2ca85dc20>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_target_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = SOK_Agent()\n",
    "evaluation = []\n",
    "\n",
    "print('Starting training')\n",
    "\n",
    "for e in range(agent.max_episodes):\n",
    "    if e % 100 == 0 and e > 0:\n",
    "        num_s = agent.test_agent(e, stochastic=False)\n",
    "        evaluation.append(num_s)\n",
    "        if num_s > agent.solved:\n",
    "            agent.solved = num_s           \n",
    "            torch.save(agent.model.state_dict(), f\"models/ddqn2_{e}_{num_s}.pth\")\n",
    "\n",
    "    sok = agent.init_sok(e)\n",
    "    random.seed(e)\n",
    "\n",
    "    state = process_frame(sok.get_image('rgb_array'))\n",
    "    room_state = sok.room_state.copy()\n",
    "    distances = calc_distances(room_state)\n",
    "\n",
    "    for step in range(sok.max_steps):\n",
    "        action = agent.act(state)\n",
    "        if action < 4:\n",
    "            next_state, reward, done, _ = sok.step(action + 1)\n",
    "        else:\n",
    "            next_state, reward, done, _ = sok.step(action + 5)\n",
    "\n",
    "        next_state = process_frame(next_state)\n",
    "        next_room_state = sok.room_state\n",
    "\n",
    "        if not done:\n",
    "            reward += box2target_change_reward(room_state, next_room_state, distances)\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state.copy()\n",
    "        room_state = next_room_state.copy()\n",
    "\n",
    "        if (step + 1) % agent.replay_rate == 0 and step > 0:\n",
    "            agent.replay()            \n",
    "\n",
    "        if done:\n",
    "            if 3 in sok.room_state:\n",
    "                agent.copy_to_prioritized_buffer(step+1)\n",
    "                \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
